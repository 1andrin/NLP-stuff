{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS AND DRIVER SETUP\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, ElementNotVisibleException, UnexpectedAlertPresentException, NoAlertPresentException\n",
    "from selenium.common.exceptions import SessionNotCreatedException, WebDriverException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import linecache\n",
    "import numpy as np\n",
    "import datefinder\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pandas import NA\n",
    "\n",
    "chrome_driver_binary = \"/Users/andrin/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Research Assistance/Web Scraping/chromedriver\"\n",
    "\n",
    "driver = webdriver.Chrome(chrome_driver_binary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRAPING ARTICLES FUNCTION\n",
    "\n",
    "def Scrape(article_urls,scraped_articles=None):\n",
    "    \n",
    "    if scraped_articles is None:\n",
    "        scraped_articles = []\n",
    "\n",
    "\n",
    "    error_articles = []\n",
    "\n",
    "    for article_number, article in enumerate(article_urls):\n",
    "\n",
    "        driver.get(article)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        #Grab All Text Content\n",
    "        all_text = str(driver.find_element_by_xpath(\"/html/body\").text)\n",
    "\n",
    "        #Find Headline\n",
    "        try:\n",
    "            headline = str(driver.find_element_by_xpath(\"//*[@class='enHeadline']\").text).replace(\"FT.com site : \",\"\")\n",
    "        except NoSuchElementException:\n",
    "            headline = pd.NA\n",
    "            print(article_number+1,\" NO HEADLINE DETECTED - CONTINUING...\")\n",
    "            \n",
    "\n",
    "        #Find Author\n",
    "        try:\n",
    "            author = driver.find_element_by_xpath(\"//*[@class='author']\").text\n",
    "        except NoSuchElementException:\n",
    "            author = pd.NA\n",
    "            print(article_number+1,\" NO AUTHOR DETECTED - CONTINUING...\")\n",
    "        \n",
    "        #Find Text Paragraphs\n",
    "        try:\n",
    "            text_lines = driver.find_elements_by_xpath(\"//*[@class='articleParagraph enarticleParagraph']\")\n",
    "        except NoSuchElementException:\n",
    "            text_lines = pd.NA\n",
    "            print(article_number+1,\" !!! WARNING: NO TEXT CONTENT DETECTED - CONTINUING...\")\n",
    "\n",
    "        \n",
    "        #Find Date\n",
    "        dates_in_text = list(datefinder.find_dates(all_text, strict = True))\n",
    "        if len(dates_in_text) > 0:\n",
    "            date = str(dates_in_text[0])[:10]\n",
    "        else:\n",
    "            date = pd.NA\n",
    "\n",
    "        #Aggregate Article Text Contents \n",
    "        content = \"\"\n",
    "        for line in text_lines:\n",
    "            content = content + str(line.text)+\"\\n\"\n",
    "\n",
    "        #Article Did Not Load Warning\n",
    "        if \"We are unable to process your request at this time. Please try again in a few minutes.\" in str(all_text):\n",
    "            print(\"ERROR: ARTICLE DID NOT LOAD !!!\")\n",
    "            time.sleep(3)\n",
    "            driver.refresh()\n",
    "            time.sleep(3)\n",
    "            error_articles.append(article)\n",
    "            error_message = \"ARTICLE DID NOT LOAD\"\n",
    "        else:\n",
    "            error_message = pd.NA\n",
    "\n",
    "        #Create Article Element and Add to List of Scraped Articles\n",
    "        article_element = [article_number+1,headline,author,date,content,error_message]\n",
    "        #print(article_element)\n",
    "        \n",
    "        scraped_articles.append(article_element)\n",
    "\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        print(\"Article Number: \", article_number+1, \" Progress: \", round((article_number+1)*100/len(article_urls),2), \"%\")\n",
    "\n",
    "    if len(error_articles) > 0:\n",
    "        Scrape(error_articles,scraped_articles)\n",
    "\n",
    "    print(len(scraped_articles))\n",
    "            \n",
    "    return scraped_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ezproxy.uzh.ch/login?url=https://global.factiva.com/en/sess/login.asp?XSID=S004cVk3HVpZXJyMTZyMTMtM96mMTImMtmm5Ff9R9apRsJpWVFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFB'\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BANK TO SEARCH FOR:  Fifth Third Bank\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrin/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Research Assistance/Web Scraping/AutomatedFactiva.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrin/Library/CloudStorage/OneDrive-Universit%C3%A4tZ%C3%BCrichUZH/Research%20Assistance/Web%20Scraping/AutomatedFactiva.ipynb#W2sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m select_daterange \u001b[39m=\u001b[39m Select(driver\u001b[39m.\u001b[39mfind_element_by_id(\u001b[39m'\u001b[39m\u001b[39mdr\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrin/Library/CloudStorage/OneDrive-Universit%C3%A4tZ%C3%BCrichUZH/Research%20Assistance/Web%20Scraping/AutomatedFactiva.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m select_daterange\u001b[39m.\u001b[39mselect_by_visible_text(\u001b[39m'\u001b[39m\u001b[39mAll Dates\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andrin/Library/CloudStorage/OneDrive-Universit%C3%A4tZ%C3%BCrichUZH/Research%20Assistance/Web%20Scraping/AutomatedFactiva.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m WebDriverWait(driver, \u001b[39m5\u001b[39;49m)\u001b[39m.\u001b[39;49muntil(EC\u001b[39m.\u001b[39;49melement_to_be_clickable((By\u001b[39m.\u001b[39;49mXPATH, \u001b[39m'\u001b[39;49m\u001b[39m//button[text()=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAll Publications\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m]\u001b[39;49m\u001b[39m'\u001b[39;49m)))\u001b[39m.\u001b[39mclick()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrin/Library/CloudStorage/OneDrive-Universit%C3%A4tZ%C3%BCrichUZH/Research%20Assistance/Web%20Scraping/AutomatedFactiva.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m#select_daterange.select_by_visible_text('Remove')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrin/Library/CloudStorage/OneDrive-Universit%C3%A4tZ%C3%BCrichUZH/Research%20Assistance/Web%20Scraping/AutomatedFactiva.ipynb#W2sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m focused_elem\u001b[39m.\u001b[39msend_keys(Keys\u001b[39m.\u001b[39mENTER)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/PY_DEV/lib/python3.9/site-packages/selenium/webdriver/support/wait.py:80\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m end_time:\n\u001b[1;32m     79\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "url = 'https://ezproxy.uzh.ch/login?url=https://global.factiva.com/en/sess/login.asp?XSID=S004cVk3HVpZXJyMTZyMTMtM96mMTImMtmm5Ff9R9apRsJpWVFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFB'\n",
    "\n",
    "all_articles_aggregate_df = pd.DataFrame(columns=['ARTICLE_NUMBER','TITLE','AUTHOR', 'DATE','TEXT','ERRORS','BANK'])\n",
    "\n",
    "banks_to_search = [\n",
    "#\"BB&T Corp\",\n",
    "#\"Bank of America\",\n",
    "#\"Bank of New York Mellon CIT Group\",\n",
    "#\"Citi\",\n",
    "#\"Citizens Financial Group\",\n",
    "#\"Comerica Inc\",\n",
    "\"Fifth Third Bank\",\n",
    "\"First Bank\",\n",
    "\"General Electric Capital\",\n",
    "#\"Goldman Sachs\",\n",
    "\"Hibernia Corp\",\n",
    "\"Huntington Bancshares\",\n",
    "\"JP Morgan\",\n",
    "\"Jefferies\",\n",
    "\"Key Bank\",\n",
    "\"M&T Bank\",\n",
    "\"Merrill Lynch International\",\n",
    "\"Morgan Stanley\",\n",
    "\"Morgan Stanley MUFG Loan Partners LLC Northern Trust Corp\",\n",
    "\"PNC Bank\",\n",
    "\"Regions Bank\",\n",
    "\"SVB Financial Group\",\n",
    "\"SouthTrust Bank\",\n",
    "\"State Street Bank\",\n",
    "\"SunTrust Bank\",\n",
    "\"US Bancorp\",\n",
    "\"Wells Fargo\",\n",
    "\"Zions Bancorporation\"]\n",
    "\n",
    "for bankname in banks_to_search:\n",
    "    print(\"BANK TO SEARCH FOR: \", bankname)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    search = True\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    focused_elem = driver.switch_to.active_element\n",
    "    search_query = bankname + \" and \" + \"climate change\"\n",
    "    focused_elem.send_keys(search_query)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    select_dupli = Select(driver.find_element_by_id('isrd'))\n",
    "    select_dupli.select_by_visible_text('Identical')\n",
    "\n",
    "    select_daterange = Select(driver.find_element_by_id('dr'))\n",
    "    select_daterange.select_by_visible_text('All Dates')\n",
    "\n",
    "    WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//button[text()=\"All Publications\"]'))).click()\n",
    "    #select_daterange.select_by_visible_text('Remove')\n",
    "\n",
    "    focused_elem.send_keys(Keys.ENTER)\n",
    "    try:\n",
    "        #WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//li[@title='Financial Times (Available through Third Party Subscription Services) - All sources']\"))).click()\n",
    "        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//li[@title='The Wall Street Journal - All sources']\"))).click()\n",
    "    except TimeoutException as te:\n",
    "        time.sleep(2)\n",
    "        print(\"NO ARTICLES FROM THIS SOURCE\")\n",
    "        search = False\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    if search == True:\n",
    "\n",
    "        inspect_urls = []\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "                for elem in elems:\n",
    "                    inspect_urls.append(elem.get_attribute(\"href\"))\n",
    "\n",
    "                time.sleep(5)\n",
    "\n",
    "                driver.execute_script(\"return arguments[0].scrollIntoView(true);\", WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.nextItem\"))))\n",
    "                driver.find_element(By.CSS_SELECTOR, 'a.nextItem').click()\n",
    "                print(\"Navigating to Next Page\")\n",
    "\n",
    "                time.sleep(5)\n",
    "\n",
    "            except UnexpectedAlertPresentException as uap:\n",
    "                try:\n",
    "                    driver.switch_to.alert.accept()\n",
    "                    print(uap)\n",
    "                    driver.refresh()\n",
    "                except NoAlertPresentException as nap:\n",
    "                    print(nap)\n",
    "                    driver.refresh()\n",
    "\n",
    "                \n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                time.sleep(2)\n",
    "                print(\"Last page reached\")\n",
    "                break\n",
    "\n",
    "        all_article_links = []\n",
    "\n",
    "        for link in inspect_urls:\n",
    "            if \"accessionno\" in link:\n",
    "                all_article_links.append(link)\n",
    "\n",
    "        all_articles = np.array(all_article_links)\n",
    "\n",
    "\n",
    "        short_links = []\n",
    "\n",
    "        for link in all_article_links:\n",
    "            short_link = link[:79]\n",
    "            short_links.append(short_link)\n",
    "\n",
    "\n",
    "        unique_indices = sorted(np.unique(short_links, return_index=True)[1].tolist())\n",
    "\n",
    "        unique_articles = all_articles[unique_indices].tolist()\n",
    "\n",
    "        #### SCRAPE TEXT\n",
    "\n",
    "        #test_articles_to_find = unique_articles[:3]\n",
    "\n",
    "        article_list_scraped = Scrape(unique_articles)\n",
    "        articles_df = pd.DataFrame(article_list_scraped)\n",
    "\n",
    "\n",
    "        articles_df.columns = ['ARTICLE_NUMBER','TITLE','AUTHOR', 'DATE','TEXT','ERRORS']\n",
    "\n",
    "        articles_df['BANK'] =  bankname\n",
    "\n",
    "        all_articles_aggregate_df = pd.concat([all_articles_aggregate_df,articles_df],ignore_index=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "all_articles_aggregate_df = all_articles_aggregate_df.drop(all_articles_aggregate_df.columns[[5]] , axis=1)\n",
    "all_articles_aggregate_df.to_csv(\"US_BANKS_TESTS.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_aggregate_df = all_articles_aggregate_df.drop(all_articles_aggregate_df.columns[[5]] , axis=1)\n",
    "all_articles_aggregate_df.to_csv(\"US_BANKS_TESTS2.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p1 = pd.read_csv(\"US_BANKS_P1.csv\")\n",
    "df_p2 = pd.read_csv(\"US_BANKS.csv\")\n",
    "\n",
    "DF_US_BANKS = pd.concat([df_p1,df_p2],ignore_index=True)\n",
    "DF_US_BANKS.to_csv(\"US_BANKS_ALL.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_articles[1][:166])\n",
    "\n",
    "short_links = []\n",
    "\n",
    "for link in all_article_links:\n",
    "    short_link = link[:166]\n",
    "    short_links.append(short_link)\n",
    "\n",
    "\n",
    "unique_indices = sorted(np.unique(short_links, return_index=True)[1].tolist())\n",
    "\n",
    "unique_articles = all_articles[unique_indices].tolist()\n",
    "\n",
    "print(len(unique_articles))\n",
    "print(unique_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AGGREGATING THE DATA FOR ALL BANKS EVER SEARCHED FOR\n",
    "\n",
    "art_df_firstpart = pd.read_csv(\"BANKS_FACTIVA.csv\")\n",
    "art_df_firstpart = art_df_firstpart.drop(art_df_firstpart.columns[[0]] , axis=1)\n",
    "\n",
    "art_df_secondpart = pd.read_csv(\"all_articles_aggregate_df1.csv\")\n",
    "\n",
    "all_banks = pd.concat([art_df_firstpart,art_df_secondpart,all_articles_aggregate_df],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_banks.to_csv(\"ALL_BANKS.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL BANKS TO ADDITIONALLY SEARCH FOR\n",
    "\n",
    "banks_to_search = [\"Royal Bank of Scotland\",\n",
    "\"Bank of Scotland\",\n",
    "\"National Westminster\",\n",
    "\"Lloyds Bank\",\n",
    "\"CIT Bank\",\n",
    "\"Midland Bank\",\n",
    "\"European Bank for Reconstruction & Development\",\n",
    "\"Allied Irish Bank\",\n",
    "\"Rabobank\",\n",
    "\"Standard Bank\",\n",
    "\"Scotiabank Europe\",\n",
    "\"Barclays de Zoete Wedd\",\n",
    "\"Nomura Bank\",\n",
    "\"Abbey National Treasury Services\",\n",
    "\"Investec Bank\",\n",
    "\"Santander UK\",\n",
    "\"Greenwich NatWest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks_to_search = [\n",
    "\"CIT Bank\",\n",
    "\"Midland Bank\",\n",
    "\"European Bank for Reconstruction & Development\",\n",
    "\"Allied Irish Bank\",\n",
    "\"Rabobank\",\n",
    "\"Standard Bank\",\n",
    "\"Scotiabank Europe\",\n",
    "\"Barclays de Zoete Wedd\",\n",
    "\"Nomura Bank\",\n",
    "\"Abbey National Treasury Services\",\n",
    "\"Investec Bank\",\n",
    "\"Santander UK\",\n",
    "\"Greenwich NatWest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks_to_search = [\"BB&T Corp\",\n",
    "\"Bank of America\",\n",
    "\"Bank of New York Mellon CIT Group\",\n",
    "\"Citi\",\n",
    "\"Citizens Financial Group\",\n",
    "\"Comerica Inc\",\n",
    "\"Fifth Third Bank\",\n",
    "\"First Bank\",\n",
    "\"General Electric Capital\",\n",
    "\"Goldman Sachs\",\n",
    "\"Hibernia Corp\",\n",
    "\"Huntington Bancshares\",\n",
    "\"JP Morgan\",\n",
    "\"Jefferies\",\n",
    "\"Key Bank\",\n",
    "\"M&T Bank\",\n",
    "\"Merrill Lynch International\",\n",
    "\"Morgan Stanley\",\n",
    "\"Morgan Stanley MUFG Loan Partners LLC Northern Trust Corp\",\n",
    "\"PNC Bank\",\n",
    "\"Regions Bank\",\n",
    "\"SVB Financial Group\",\n",
    "\"SouthTrust Bank\",\n",
    "\"State Street Bank\",\n",
    "\"SunTrust Bank\",\n",
    "\"US Bancorp\",\n",
    "\"Wells Fargo\",\n",
    "\"Zions Bancorporation\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('PY_DEV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a297dc86dc48aa590fd946072e279fe89d794c229ff04b7fb52ffcc4b0fc9ed5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
