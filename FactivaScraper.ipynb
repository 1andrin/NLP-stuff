{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS AND DRIVER SETUP\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, ElementNotVisibleException, UnexpectedAlertPresentException, NoAlertPresentException\n",
    "from selenium.common.exceptions import SessionNotCreatedException, WebDriverException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import linecache\n",
    "import numpy as np\n",
    "import datefinder\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pandas import NA\n",
    "\n",
    "chrome_driver_binary = \"/Users/andrin/Library/CloudStorage/OneDrive-LondonSchoolofEconomics/DS Project/chromedriver\"\n",
    "\n",
    "driver = webdriver.Chrome(chrome_driver_binary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://librarysearch.lse.ac.uk/view/action/uresolver.do?operation=resolveService&package_service_id=10444710030002021&institutionId=2021&customerId=2020&VE=true'\n",
    "\n",
    "spec_url = \"https://global-factiva-com.gate3.library.lse.ac.uk/ha/default.aspx?page_driver=searchBuilder_Search#./!?&_suid=1699211604258013312993362481285\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_urls = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "        for elem in elems:\n",
    "            inspect_urls.append(elem.get_attribute(\"href\"))\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        driver.execute_script(\"return arguments[0].scrollIntoView(true);\", WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.nextItem\"))))\n",
    "        driver.find_element(By.CSS_SELECTOR, 'a.nextItem').click()\n",
    "        print(\"Navigating to Next Page\")\n",
    "\n",
    "        time.sleep(15)\n",
    "\n",
    "    except UnexpectedAlertPresentException as uap:\n",
    "        try:\n",
    "            driver.switch_to.alert.accept()\n",
    "            print(uap)\n",
    "            driver.refresh()\n",
    "        except NoAlertPresentException as nap:\n",
    "            print(nap)\n",
    "            driver.refresh()\n",
    "\n",
    "        \n",
    "    except (TimeoutException, WebDriverException) as e:\n",
    "        time.sleep(2)\n",
    "        print(\"Last page reached\")\n",
    "        break\n",
    "\n",
    "display(inspect_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_article_links = []\n",
    "\n",
    "for link in inspect_urls:\n",
    "    if \"accessionno\" in link:\n",
    "        all_article_links.append(link)\n",
    "\n",
    "\n",
    "short_links = []\n",
    "\n",
    "for link in all_article_links:\n",
    "    short_link = re.sub(\"&fcpil=en&napc=S&sa_from=&cat=a&page_driver=searchBuilder_Search\", '', link)   \n",
    "    short_link = re.sub(\"https://global-factiva-com.gate3.library.lse.ac.uk/du/article.aspx/?accessionno=BIZINS0020231103ejb3001bb&drn=drn:archive.newsarticle.\", '', link)\n",
    "    short_links.append(short_link)\n",
    "\n",
    "\n",
    "unique_indices = sorted(np.unique(short_links, return_index=True)[1].tolist())\n",
    "\n",
    "unique_articles = []\n",
    "\n",
    "for i in unique_indices:\n",
    "    unique_articles.append(all_article_links[i])\n",
    "\n",
    "\n",
    "display(unique_articles)\n",
    "print(len(unique_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRAPING ARTICLES FUNCTION\n",
    "\n",
    "def Scrape(article_urls,scraped_articles=None):\n",
    "    \n",
    "    if scraped_articles is None:\n",
    "        scraped_articles = []\n",
    "\n",
    "\n",
    "    error_articles = []\n",
    "\n",
    "    for article_number, article in enumerate(article_urls):\n",
    "        try:\n",
    "            driver.get(article)\n",
    "        except TimeoutException:\n",
    "            time.sleep(3)\n",
    "            driver.refresh()\n",
    "            time.sleep(60)\n",
    "            driver.get(article)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        #Grab All Text Content\n",
    "        try:\n",
    "            all_text = str(driver.find_element_by_xpath(\"/html/body\").text)\n",
    "        except NoSuchElementException:\n",
    "            all_text = \"We are unable to process your request at this time. Please try again in a few minutes.\"\n",
    "            print(article_number+1,\" !!! WARNING: BLANK PAGE --------------------------------------------------\")\n",
    "\n",
    "        #Find Headline\n",
    "        try:\n",
    "            headline = str(driver.find_element_by_xpath(\"//*[@class='enHeadline']\").text).replace(\"FT.com site : \",\"\")\n",
    "        except NoSuchElementException:\n",
    "            headline = pd.NA\n",
    "            print(article_number+1,\" NO HEADLINE DETECTED - CONTINUING...\")\n",
    "            \n",
    "\n",
    "        #Find Author\n",
    "        try:\n",
    "            author = driver.find_element_by_xpath(\"//*[@class='author']\").text\n",
    "        except NoSuchElementException:\n",
    "            author = pd.NA\n",
    "            print(article_number+1,\" NO AUTHOR DETECTED - CONTINUING...\")\n",
    "        \n",
    "        #Find Text Paragraphs\n",
    "        try:\n",
    "            text_lines = driver.find_elements_by_xpath(\"//*[@class='articleParagraph enarticleParagraph']\")\n",
    "        except NoSuchElementException:\n",
    "            text_lines = pd.NA\n",
    "            print(article_number+1,\" !!! WARNING: NO TEXT CONTENT DETECTED - CONTINUING...\")\n",
    "\n",
    "        \n",
    "        #Find Date\n",
    "        dates_in_text = list(datefinder.find_dates(all_text, strict = True))\n",
    "        if len(dates_in_text) > 0:\n",
    "            date = str(dates_in_text[0])[:10]\n",
    "        else:\n",
    "            date = pd.NA\n",
    "\n",
    "        #Aggregate Article Text Contents \n",
    "        content = \"\"\n",
    "        for line in text_lines:\n",
    "            content = content + str(line.text)+\"\\n\"\n",
    "\n",
    "        #Article Did Not Load Warning\n",
    "        if \"We are unable to process your request at this time. Please try again in a few minutes.\" in str(all_text):\n",
    "            print(\"ERROR: ARTICLE DID NOT LOAD !!!\")\n",
    "            time.sleep(3)\n",
    "            driver.refresh()\n",
    "            time.sleep(3)\n",
    "            error_articles.append(article)\n",
    "            error_message = \"ARTICLE DID NOT LOAD\"\n",
    "        else:\n",
    "            error_message = pd.NA\n",
    "\n",
    "        #Create Article Element and Add to List of Scraped Articles\n",
    "        article_element = [article_number+1,headline,author,date,content,error_message]\n",
    "        #print(article_element)\n",
    "        \n",
    "        scraped_articles.append(article_element)\n",
    "\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        print(\"Article Number: \", article_number+1, \" Progress: \", round((article_number+1)*100/len(article_urls),2), \"%\")\n",
    "\n",
    "    if len(error_articles) > 0:\n",
    "        Scrape(error_articles,scraped_articles)\n",
    "\n",
    "    print(len(scraped_articles))\n",
    "            \n",
    "    return scraped_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list_scraped = Scrape(unique_articles)\n",
    "articles_df = pd.DataFrame(article_list_scraped)\n",
    "\n",
    "articles_df.columns = ['ARTICLE_NUMBER','TITLE','AUTHOR', 'DATE','TEXT','ERRORS']\n",
    "\n",
    "articles_df.to_csv(\"Factiva_NYT.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StandardKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
